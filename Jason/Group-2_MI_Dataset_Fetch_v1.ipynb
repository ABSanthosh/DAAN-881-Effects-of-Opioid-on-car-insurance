{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8cd05d-b40c-413f-82d5-92f3bdbe078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta, datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException,\n",
    "    TimeoutException,\n",
    "    ElementNotInteractableException,\n",
    "    StaleElementReferenceException,\n",
    "    WebDriverException\n",
    ")\n",
    "from bs4 import BeautifulSoup\n",
    "# Using webdriver-manager simplifies driver setup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd5e7b-9c82-45e5-862d-a9e3b00754b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MTCF_URL = \"https://www.michigantrafficcrashfacts.org/data/querytool\"\n",
    "START_YEAR = 2013\n",
    "END_YEAR = 2022\n",
    "OUTPUT_DIR = \"mtcf_scraped_data_person_level_daily\"\n",
    "\n",
    "# Delays to minimize risk of blocking (adjust as needed, longer is safer)\n",
    "# Total runtime will be roughly (MIN_DELAY + MAX_DELAY)/2 * 365 * 10 seconds\n",
    "MIN_DELAY_SECONDS = 45  # Minimum delay between processing each day\n",
    "MAX_DELAY_SECONDS = 120 # Maximum delay between processing each day\n",
    "PAGE_LOAD_TIMEOUT = 60  # Max seconds to wait for page elements\n",
    "RETRY_MAX_ATTEMPTS = 3\n",
    "RETRY_BASE_DELAY_SECONDS = 30 # Initial delay for retries, increases exponentially\n",
    "\n",
    "# --- Logging Setup ---\n",
    "LOG_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "LOG_FILENAME = 'mtcf_scraper.log'\n",
    "\n",
    "# Ensure handlers are not added multiple times if script is re-run in same session\n",
    "logger = logging.getLogger()\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(LOG_FILENAME)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(logging.Formatter(LOG_FORMAT))\n",
    "    logger.addHandler(file_handler)\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(logging.Formatter(LOG_FORMAT))\n",
    "    logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9078f8-b431-4b76-ab57-2b0261f6d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def get_api_field_names(user_attribute_list, api_metadata_fields):\n",
    "    \"\"\"\n",
    "    Maps user-friendly attribute names to actual API field names found in metadata.\n",
    "\n",
    "    TODO: This function provides a basic mapping attempt. You MUST review\n",
    "          the actual API field names from the metadata (FEATURE_SERVICE_URL + '?f=json')\n",
    "          and update this logic or the 'mapping' dictionary below to ensure accuracy.\n",
    "          Pay close attention to case sensitivity and exact naming conventions.\n",
    "    \"\"\"\n",
    "    # --- Placeholder Mapping (Update based on actual metadata) ---\n",
    "    # Add more known mappings here as you discover them from the API metadata\n",
    "    mapping = {\n",
    "        \"Crash Year\": \"CRASH_YEAR\",\n",
    "        \"County\": \"COUNTY_NAME\",\n",
    "        \"Crash Month\": \"CRASH_MONTH\",\n",
    "        \"Crash Day\": \"CRASH_DAY\",\n",
    "        \"Person Age\": \"PERSON_AGE\",\n",
    "        \"Person Gender\": \"PERSON_GENDER\",\n",
    "        \"Person Degree of Injury\": \"PERSON_INJURY_SEVERITY\",\n",
    "        \"Driver Age\": \"DRIVER_AGE\",\n",
    "        \"Driver Gender\": \"DRIVER_GENDER\",\n",
    "        \"Worst Injury in Crash\": \"MAX_INJURY_SEVERITY\",\n",
    "        \"Vehicle Type\": \"VEHICLE_TYPE\",\n",
    "        \"City or Township\": \"CITY_TOWNSHIP_NAME\",\n",
    "        \"Day of Week\": \"DAY_OF_WEEK\",\n",
    "        \"Lighting Conditions\": \"LIGHTING_CONDITION\",\n",
    "        \"Road Conditions\": \"ROAD_CONDITION\",\n",
    "        \"Weather Conditions (2016+)\": \"WEATHER_CONDITION\", # Assuming one field covers recent weather\n",
    "        \"Time of Day\": \"CRASH_TIME\", # Often part of a datetime field\n",
    "        \"Posted Speed Limit\": \"POSTED_SPEED_LIMIT\",\n",
    "        \"Traffic Control\": \"TRAFFIC_CONTROL_TYPE\",\n",
    "        \"Worst Injury in Unit\": \"UNIT_MAX_INJURY_SEVERITY\",\n",
    "        \"Party Type\": \"PARTY_TYPE\",\n",
    "        \"Person Position\": \"PERSON_POSITION\",\n",
    "        \"Person Ejection\": \"PERSON_EJECTION\",\n",
    "        \"Person Restraint\": \"PERSON_RESTRAINT_USED\",\n",
    "        \"Driver Drinking\": \"DRIVER_DRINKING_INDICATOR\",\n",
    "        \"Crash: Drinking\": \"ALCOHOL_INVOLVED_CRASH\", # Example guess\n",
    "        \"Crash: Drug Use\": \"DRUG_INVOLVED_CRASH\", # Example guess\n",
    "        \"Crash: Fatal Crash\": \"FATAL_CRASH_INDICATOR\",\n",
    "        \"Crash: Injury Crash\": \"INJURY_CRASH_INDICATOR\",\n",
    "        \"Crash Type\": \"CRASH_TYPE_DESC\",\n",
    "        #... add ALL other required fields based on API metadata inspection...\n",
    "    }\n",
    "\n",
    "    api_fields_to_request = set()\n",
    "    missing_attributes =\n",
    "    available_api_field_names_map = {field['name'].upper(): field['name'] for field in api_metadata_fields} # Map upper case to original case\n",
    "\n",
    "    print(\"\\n--- Attribute Mapping Attempt ---\")\n",
    "    for user_attr in user_attribute_list:\n",
    "        found = False\n",
    "        # 1. Try predefined mapping (case-insensitive key check)\n",
    "        if user_attr in mapping:\n",
    "            mapped_name_upper = mapping[user_attr].upper()\n",
    "            if mapped_name_upper in available_api_field_names_map:\n",
    "                api_fields_to_request.add(available_api_field_names_map[mapped_name_upper])\n",
    "                print(f\"  Mapped '{user_attr}' -> '{available_api_field_names_map[mapped_name_upper]}' (predefined)\")\n",
    "                found = True\n",
    "\n",
    "        # 2. Try direct name match (case-insensitive, ignoring spaces/punctuation/years)\n",
    "        if not found:\n",
    "            normalized_user_attr = ''.join(filter(str.isalnum, user_attr.split('('))).upper()\n",
    "            for api_name_upper, original_api_name in available_api_field_names_map.items():\n",
    "                normalized_api_name = ''.join(filter(str.isalnum, original_api_name)).upper()\n",
    "                if normalized_user_attr == normalized_api_name:\n",
    "                    api_fields_to_request.add(original_api_name)\n",
    "                    print(f\"  Mapped '{user_attr}' -> '{original_api_name}' (normalized name match)\")\n",
    "                    found = True\n",
    "                    break # Found a match\n",
    "\n",
    "        if not found:\n",
    "            missing_attributes.append(user_attr)\n",
    "            # print(f\"  Warning: Could not map user attribute '{user_attr}'.\") # Keep output cleaner\n",
    "\n",
    "    if missing_attributes:\n",
    "        print(\"\\n--- Attributes Not Found/Mapped ---\")\n",
    "        for attr in missing_attributes:\n",
    "            print(f\"  - {attr}\")\n",
    "        print(\"  Please inspect API metadata and update the 'mapping' dictionary or logic.\")\n",
    "        print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "    if not api_fields_to_request:\n",
    "         print(\"\\nError: No attributes could be mapped. Cannot proceed without knowing which fields to request.\")\n",
    "         print(\"Please update the get_api_field_names function with correct mappings based on API metadata.\")\n",
    "         return None, missing_attributes # Indicate failure\n",
    "\n",
    "    # Ensure essential fields for filtering and pagination are included\n",
    "    # TODO: Verify these default names ('OBJECTID', 'CRASH_YEAR', 'CRASH_DATE') against metadata\n",
    "    oid_field_default = \"OBJECTID\"\n",
    "    year_field_default = \"CRASH_YEAR\"\n",
    "    date_field_default = \"CRASH_DATE\"\n",
    "\n",
    "    essential_fields_to_add = set()\n",
    "    found_oid = False\n",
    "    found_year = False\n",
    "    found_date = False\n",
    "\n",
    "    for api_name_upper, original_api_name in available_api_field_names_map.items():\n",
    "        if api_name_upper == oid_field_default.upper():\n",
    "            essential_fields_to_add.add(original_api_name)\n",
    "            found_oid = True\n",
    "        if api_name_upper == year_field_default.upper():\n",
    "            essential_fields_to_add.add(original_api_name)\n",
    "            found_year = True\n",
    "        if api_name_upper == date_field_default.upper():\n",
    "             essential_fields_to_add.add(original_api_name)\n",
    "             found_date = True\n",
    "\n",
    "    if not found_oid: print(f\"Warning: Default OBJECTID field '{oid_field_default}' not found. Pagination might fail.\")\n",
    "    if not found_year and not found_date: print(f\"Warning: Default Year ('{year_field_default}') or Date ('{date_field_default}') field not found. Filtering will likely fail.\")\n",
    "\n",
    "    final_field_list = list(api_fields_to_request.union(essential_fields_to_add))\n",
    "    print(f\"\\nFinal list of API fields to request: {', '.join(final_field_list)}\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "    return final_field_list, missing_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8cf20-1900-443b-878d-fa88e664694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Placeholder Selectors (MUST BE VERIFIED/UPDATED BY INSPECTING THE LIVE SITE) ---\n",
    "# Use CSS Selectors (e.g., '#elementId', '.className', 'tag[attribute=\"value\"]')\n",
    "# Or XPath (less preferred due to fragility)\n",
    "ANALYSIS_LEVEL_DROPDOWN_SELECTOR = 'select[aria-label=\"Analysis Level\"]' # Placeholder - VERIFY\n",
    "PERSONS_OPTION_VALUE = \"persons\" # Placeholder value for the 'Persons' option - VERIFY\n",
    "START_DATE_INPUT_SELECTOR = '#input-41' # Placeholder - VERIFY\n",
    "END_DATE_INPUT_SELECTOR = '#input-44[aria-label=\"End Date\"]' # Placeholder - VERIFY\n",
    "# Date picker interaction might be complex - inspect how dates are selected (e.g., calendar clicks, direct input)\n",
    "SUBMIT_BUTTON_SELECTOR = 'button:contains(\"Update Query\")' # Placeholder - VERIFY (This might need adjustment based on actual text/structure)\n",
    "RESULTS_TABLE_SELECTOR = 'table.dataTable' # Placeholder - VERIFY (Look for a table with results)\n",
    "RESULTS_TABLE_HEADER_SELECTOR = 'thead tr th' # Placeholder - VERIFY\n",
    "RESULTS_TABLE_BODY_SELECTOR = 'tbody' # Placeholder - VERIFY\n",
    "RESULTS_TABLE_ROW_SELECTOR = 'tr' # Placeholder - VERIFY\n",
    "RESULTS_TABLE_CELL_SELECTOR = 'td' # Placeholder - VERIFY\n",
    "PAGINATION_NEXT_BUTTON_SELECTOR = 'a.paginate_button.next:not(.disabled)' # Placeholder - VERIFY (Check for enabled 'next' link)\n",
    "# Element indicating results have loaded/updated (e.g., a results count span, or the table itself)\n",
    "RESULTS_LOADED_INDICATOR_SELECTOR = RESULTS_TABLE_SELECTOR # Placeholder - VERIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4fcc6b-c55c-48bf-a256-4357900750aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Sets up the Selenium WebDriver.\"\"\"\n",
    "    options = Options()\n",
    "    # options.add_argument(\"--headless\")  # Run headless (without opening browser window) - may be detected more easily\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    # Rotate User Agent (optional, add more legitimate agents)\n",
    "    user_agents =\n",
    "    options.add_argument(f'user-agent={random.choice(user_agents)}')\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging']) # Suppress DevTools messages\n",
    "\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        logging.info(\"WebDriver initialized successfully.\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Failed to initialize WebDriver: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def set_date_fields_simple_input(driver, wait, date_selector, target_date_str):\n",
    "    \"\"\"Attempts to set date by clearing and sending keys. May need adjustment.\"\"\"\n",
    "    try:\n",
    "        date_input = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, date_selector)))\n",
    "        date_input.clear()\n",
    "        # Sometimes clicking first helps activate fields\n",
    "        date_input.click()\n",
    "        time.sleep(0.5)\n",
    "        date_input.send_keys(target_date_str)\n",
    "        # Add a check to see if the value was set correctly if possible\n",
    "        time.sleep(0.5) # Pause after sending keys\n",
    "        logging.info(f\"Set date field {date_selector} to {target_date_str}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not set date field {date_selector} to {target_date_str}: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_data_from_html(html_content, expected_headers=None):\n",
    "    \"\"\"Parses HTML table content using BeautifulSoup.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "    table = soup.select_one(RESULTS_TABLE_SELECTOR)\n",
    "    if not table:\n",
    "        logging.warning(\"Results table not found in HTML content.\")\n",
    "        return expected_headers if expected_headers else, # Return empty data, keep headers if known\n",
    "\n",
    "    headers =\n",
    "    header_elements = table.select(RESULTS_TABLE_HEADER_SELECTOR)\n",
    "    if header_elements:\n",
    "        headers =\n",
    "    elif expected_headers:\n",
    "         logging.warning(\"Table header row not found, using previously known headers.\")\n",
    "         headers = expected_headers\n",
    "    else:\n",
    "        logging.warning(\"Could not extract table headers.\")\n",
    "        # Attempt fallback: get headers from first data row if necessary (less reliable)\n",
    "\n",
    "    data_records =\n",
    "    table_body = table.select_one(RESULTS_TABLE_BODY_SELECTOR)\n",
    "    if table_body:\n",
    "        rows = table_body.select(RESULTS_TABLE_ROW_SELECTOR)\n",
    "        for row in rows:\n",
    "            cells =\n",
    "            # Basic validation: check if row looks like data (adjust condition if needed)\n",
    "            if cells and len(cells) == len(headers):\n",
    "                 record = dict(zip(headers, cells))\n",
    "                 data_records.append(record)\n",
    "            elif cells: # Log if cell count mismatch only if cells were found\n",
    "                 logging.warning(f\"Row skipped due to mismatch: {len(cells)} cells vs {len(headers)} headers. Row data: {cells}\")\n",
    "    else:\n",
    "        logging.warning(\"Table body not found.\")\n",
    "\n",
    "    # If headers were just extracted, return them. Otherwise, return the ones passed in.\n",
    "    final_headers = headers if headers else expected_headers if expected_headers else\n",
    "    return final_headers, data_records\n",
    "\n",
    "def safe_click(driver, wait, selector):\n",
    "    \"\"\"Clicks an element safely with explicit wait.\"\"\"\n",
    "    try:\n",
    "        element = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector)))\n",
    "        element.click()\n",
    "        return True\n",
    "    except (TimeoutException, ElementNotInteractableException) as e:\n",
    "        logging.warning(f\"Could not click element {selector}: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error clicking element {selector}: {e}\", exc_info=True)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4999f9-c5dd-4ca8-985e-1265f88ee0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Script ---\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"=\"*50)\n",
    "    logging.info(\"Starting MTCF Scraping Script (Daily Iteration)\")\n",
    "    logging.warning(\"DISCLAIMER: This script attempts to scrape data from the MTCF Data Query Tool.\")\n",
    "    logging.warning(\"This likely violates their Terms of Use and carries a HIGH RISK of IP blocking.\")\n",
    "    logging.warning(\"Use of this script is at your own risk. Official data requests are recommended.\")\n",
    "    logging.warning(f\"Estimated runtime with delays ({MIN_DELAY_SECONDS}-{MAX_DELAY_SECONDS}s/day): VERY LONG (potentially days).\")\n",
    "    logging.info(\"Ensure all placeholder selectors (e.g., 'YOUR_SELECTOR_HERE') are updated!\")\n",
    "    logging.info(\"=\"*50)\n",
    "\n",
    "    # Basic check for placeholder URL - replace with checks for selectors if possible\n",
    "    if \"YOUR_SELECTOR_HERE\" in ANALYSIS_LEVEL_DROPDOWN_SELECTOR:\n",
    "         logging.critical(\"Placeholder selectors detected. Please update them by inspecting the MTCF website.\")\n",
    "         # exit() # Commented out to allow running even if placeholders remain, but WARN heavily\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    driver = None # Initialize driver variable\n",
    "\n",
    "    try:\n",
    "        driver = setup_driver()\n",
    "        base_wait = WebDriverWait(driver, PAGE_LOAD_TIMEOUT)\n",
    "\n",
    "        for year in range(START_YEAR, END_YEAR + 1):\n",
    "            logging.info(f\"===== Processing Year: {year} =====\")\n",
    "            yearly_data_list =\n",
    "            yearly_headers_list = None # Store headers found for the year\n",
    "            start_date_year = date(year, 1, 1)\n",
    "            end_date_year = date(year, 12, 31)\n",
    "            current_process_date = start_date_year\n",
    "\n",
    "            while current_process_date <= end_date_year:\n",
    "                day_str = current_process_date.strftime('%Y-%m-%d') # Format needed for input\n",
    "                logging.info(f\"--- Processing Date: {day_str} ---\")\n",
    "                attempt = 0\n",
    "                success = False\n",
    "\n",
    "                while attempt < RETRY_MAX_ATTEMPTS and not success:\n",
    "                    attempt += 1\n",
    "                    if attempt > 1:\n",
    "                        wait_time = RETRY_BASE_DELAY_SECONDS * (2 ** (attempt - 2)) # Exponential backoff\n",
    "                        logging.info(f\"Retry attempt {attempt}/{RETRY_MAX_ATTEMPTS}. Waiting for {wait_time} seconds...\")\n",
    "                        time.sleep(wait_time)\n",
    "\n",
    "                    try:\n",
    "                        logging.info(f\"Navigating to MTCF tool for {day_str} (Attempt {attempt})\")\n",
    "                        driver.get(MTCF_URL)\n",
    "\n",
    "                        # 1. Select Analysis Level: Persons\n",
    "                        logging.info(\"Waiting for Analysis Level dropdown...\")\n",
    "                        analysis_dropdown_element = base_wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ANALYSIS_LEVEL_DROPDOWN_SELECTOR)))\n",
    "                        select = Select(analysis_dropdown_element)\n",
    "                        select.select_by_value(PERSONS_OPTION_VALUE)\n",
    "                        logging.info(\"Selected 'Persons' analysis level.\")\n",
    "                        time.sleep(random.uniform(0.5, 1.5)) # Small pause after selection\n",
    "\n",
    "                        # 2. Set Date Range (Start and End = current_process_date)\n",
    "                        logging.info(f\"Setting Start Date to {day_str}\")\n",
    "                        if not set_date_fields_simple_input(driver, base_wait, START_DATE_INPUT_SELECTOR, day_str):\n",
    "                             raise Exception(f\"Failed to set Start Date for {day_str}\")\n",
    "                        time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "                        logging.info(f\"Setting End Date to {day_str}\")\n",
    "                        if not set_date_fields_simple_input(driver, base_wait, END_DATE_INPUT_SELECTOR, day_str):\n",
    "                             raise Exception(f\"Failed to set End Date for {day_str}\")\n",
    "                        time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "                        # 3. Submit Query\n",
    "                        logging.info(\"Submitting query...\")\n",
    "                        # Find the submit button - might need adjustment\n",
    "                        # Using JavaScript click can sometimes bypass overlay issues\n",
    "                        submit_button_element = base_wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, SUBMIT_BUTTON_SELECTOR)))\n",
    "                        driver.execute_script(\"arguments.click();\", submit_button_element)\n",
    "                        # if not safe_click(driver, base_wait, SUBMIT_BUTTON_SELECTOR):\n",
    "                        #     raise Exception(\"Failed to click submit button.\")\n",
    "                        logging.info(\"Query submitted.\")\n",
    "\n",
    "                        # 4. Wait for results to load/update\n",
    "                        logging.info(\"Waiting for results to load...\")\n",
    "                        # Wait for the table or a known element within it to be present\n",
    "                        base_wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, RESULTS_LOADED_INDICATOR_SELECTOR)))\n",
    "                        logging.info(\"Results loaded indicator found.\")\n",
    "                        # Add extra pause for safety, especially if content loads dynamically after table structure appears\n",
    "                        time.sleep(random.uniform(3, 7))\n",
    "\n",
    "                        # --- Data Extraction and Pagination for the Day ---\n",
    "                        daily_records_list =\n",
    "                        current_page = 1\n",
    "                        while True: # Loop for pagination\n",
    "                            logging.info(f\"Extracting data from page {current_page} for {day_str}...\")\n",
    "                            try:\n",
    "                                # Store reference to current table to detect changes\n",
    "                                old_table_element = driver.find_element(By.CSS_SELECTOR, RESULTS_TABLE_SELECTOR)\n",
    "                            except NoSuchElementException:\n",
    "                                logging.warning(f\"Results table disappeared before extraction on page {current_page}. Assuming no data.\")\n",
    "                                old_table_element = None # Ensure staleness check doesn't block indefinitely\n",
    "\n",
    "                            page_html = driver.page_source\n",
    "                            headers, page_data = extract_data_from_html(page_html, yearly_headers_list)\n",
    "\n",
    "                            if not yearly_headers_list and headers: # Store headers if found for the first time this year\n",
    "                                yearly_headers_list = headers\n",
    "                                logging.info(f\"Captured headers for year {year}: {yearly_headers_list}\")\n",
    "                            elif headers and yearly_headers_list and headers!= yearly_headers_list:\n",
    "                                logging.warning(f\"Header mismatch detected for {day_str}! Previous: {yearly_headers_list}, Current: {headers}\")\n",
    "                                # Decide handling: stop, use old, use new, merge? Using old for consistency this run.\n",
    "                                headers = yearly_headers_list\n",
    "\n",
    "                            if page_data:\n",
    "                                daily_records_list.extend(page_data)\n",
    "                                logging.info(f\"Extracted {len(page_data)} records from page {current_page}.\")\n",
    "                            else:\n",
    "                                logging.info(f\"No data records found on page {current_page}.\")\n",
    "\n",
    "                            # Check for and click the \"Next\" button\n",
    "                            try:\n",
    "                                next_button_element = driver.find_element(By.CSS_SELECTOR, PAGINATION_NEXT_BUTTON_SELECTOR)\n",
    "                                logging.info(\"Next page button found. Clicking...\")\n",
    "                                driver.execute_script(\"arguments.click();\", next_button_element) # JS click often more reliable\n",
    "                                current_page += 1\n",
    "\n",
    "                                # Wait for the table to update after clicking next\n",
    "                                logging.info(\"Waiting for table to update after pagination...\")\n",
    "                                if old_table_element:\n",
    "                                     base_wait.until(EC.staleness_of(old_table_element))\n",
    "                                # Add extra wait for content to actually render\n",
    "                                time.sleep(random.uniform(3, 6))\n",
    "                                base_wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, RESULTS_LOADED_INDICATOR_SELECTOR)))\n",
    "                                logging.info(\"Table updated for next page.\")\n",
    "\n",
    "                            except (NoSuchElementException, ElementNotInteractableException):\n",
    "                                logging.info(f\"No more 'Next' page button found or interactable for {day_str}. End of results for this day.\")\n",
    "                                break # Exit pagination loop\n",
    "                            except TimeoutException:\n",
    "                                 logging.warning(f\"Timeout waiting for table update after pagination click for {day_str}. Assuming end of results.\")\n",
    "                                 break # Exit pagination loop\n",
    "                            except Exception as page_e:\n",
    "                                 logging.error(f\"Unexpected error during pagination for {day_str}: {page_e}\", exc_info=True)\n",
    "                                 break # Exit pagination loop\n",
    "\n",
    "                        # --- End of Pagination Loop ---\n",
    "                        logging.info(f\"Total records extracted for {day_str}: {len(daily_records_list)}\")\n",
    "                        yearly_data_list.extend(daily_records_list)\n",
    "                        success = True # Mark day as successful\n",
    "\n",
    "                    except WebDriverException as e:\n",
    "                        logging.error(f\"WebDriverException on {day_str} (Attempt {attempt}): {e}\")\n",
    "                        if \"ERR_CONNECTION_RESET\" in str(e) or \"timeout\" in str(e).lower():\n",
    "                             logging.warning(\"Connection error detected, will retry.\")\n",
    "                        else:\n",
    "                             # If it's not a known retryable error, maybe stop retrying\n",
    "                             logging.error(\"Unknown WebDriverException, stopping retries for this day.\")\n",
    "                             attempt = RETRY_MAX_ATTEMPTS # Force break\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Unexpected error processing {day_str} (Attempt {attempt}): {e}\", exc_info=True)\n",
    "                        # Consider if driver needs restart on certain errors\n",
    "\n",
    "                # --- End of Retry Loop ---\n",
    "                if not success:\n",
    "                    logging.error(f\"Failed to process date {day_str} after {RETRY_MAX_ATTEMPTS} attempts. Skipping.\")\n",
    "\n",
    "                # Move to next day\n",
    "                current_process_date += timedelta(days=1)\n",
    "\n",
    "                # Implement the long delay between daily queries\n",
    "                delay = random.uniform(MIN_DELAY_SECONDS, MAX_DELAY_SECONDS)\n",
    "                logging.info(f\"--- Waiting for {delay:.1f} seconds before processing next day ---\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "            # --- End of Year Processing ---\n",
    "            if yearly_data_list:\n",
    "                logging.info(f\"Aggregating and saving data for year {year}...\")\n",
    "                try:\n",
    "                    # Use headers captured during the year, or let Pandas infer if none found\n",
    "                    final_yearly_df = pd.DataFrame(yearly_data_list, columns=yearly_headers_list)\n",
    "\n",
    "                    output_filename = os.path.join(OUTPUT_DIR, f\"mtcf_person_data_{year}.csv\")\n",
    "                    final_yearly_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "                    logging.info(f\"Successfully saved {len(final_yearly_df)} records for {year} to {output_filename}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to create or save DataFrame for year {year}: {e}\", exc_info=True)\n",
    "            else:\n",
    "                logging.warning(f\"No data collected for year {year}.\")\n",
    "\n",
    "            logging.info(f\"===== Finished processing year: {year} =====\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "         logging.warning(\"Script interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"A critical error occurred during the scraping process: {e}\", exc_info=True)\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            logging.info(\"WebDriver closed.\")\n",
    "        logging.info(\"=\"*50)\n",
    "        logging.info(\"Script finished.\")\n",
    "        logging.info(f\"Log file saved to: {LOG_FILENAME}\")\n",
    "        logging.info(f\"CSV files (if any) saved in directory: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "        logging.warning(\"REMINDER: Review the log file for any errors or warnings.\")\n",
    "        logging.warning(\"REMINDER: Scraping MTCF is against their Terms of Use and may lead to blocking.\")\n",
    "        logging.info(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b92ec-10c5-415e-8e15-194eb69a26cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
